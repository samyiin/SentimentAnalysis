{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1bd62ea6e7ac25",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Bidirectional LSTM with word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b83d92e571fa1f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Representation\n",
    "The input to our model is a sentence string, we will represent a sentence by word2vec vector. The shape is (300,), type is float64.   \n",
    "Need to use gensim 4.3.2 (current version) will \"from scipy.linalg import get_blas_funcs, triu\", and triu is removed from scipy 1.12. And we can't install scipy 1.11. I tried to download the file and write a load function for it, but there is little information on the internet, everyone is using gensim.  \n",
    "So eventually my solution is to use gensim 4.3.2 and python 3.10. We can install scipy 1.11.0 with python 3.10, and it solves the problem. besides, gensim.downloader.load(\"word2vec-google-news-300\") seems stop working. So we will have to download the file from\n",
    " https://code.google.com/archive/p/word2vec/    \n",
    " (1.5 GB) and unzip it (not sure if it's necessary), and then use from gensim.models import KeyedVectors to solve this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "159cc923df98a7ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:12.004732Z",
     "start_time": "2024-05-21T21:21:12.000769Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:12.786901Z",
     "start_time": "2024-05-21T21:21:12.009439Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "W2V_EMBEDDING_DIM = 300\n",
    "SEQ_LEN = 52\n",
    "\n",
    "def load_word2vec():\n",
    "    # word2vec_model = gensim.downloader.load(\"word2vec-google-news-300\") \n",
    "    # above method doesn't work anymore, you need to download the file from internet\n",
    "    word2vec_file = 'TempFiles/GoogleNews-vectors-negative300.bin'\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\n",
    "    return word2vec_model\n",
    "\n",
    "def create_or_load_slim_w2v(words_list, cache_w2v=True):\n",
    "    \"\"\"\n",
    "    We are trying to get a smaller word2vec dictionary: word2vec dict only for words which appear in the training dataset.\n",
    "    :param words_list: list of words to use for the w2v dict\n",
    "    :param cache_w2v: whether to save locally the small w2v dictionary\n",
    "    :return: dictionary which maps the known words to their vectors\n",
    "    \"\"\"\n",
    "    w2v_path = \"TempFiles/w2v_dict.pkl\"\n",
    "    if not os.path.exists(w2v_path):\n",
    "        full_w2v = load_word2vec()\n",
    "        w2v_emb_dict = {k: full_w2v[k] for k in words_list if k in full_w2v}\n",
    "        if cache_w2v:\n",
    "            save_pickle(w2v_emb_dict, w2v_path)\n",
    "    else:\n",
    "        w2v_emb_dict = load_pickle(w2v_path)\n",
    "    return w2v_emb_dict\n",
    "\n",
    "\n",
    "def sentence_to_embedding(sent, word_to_vec, seq_len=SEQ_LEN, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    this method gets a sentence and a word to vector mapping, and returns a list containing the\n",
    "    words embeddings of the tokens in the sentence.\n",
    "    :param sent: a list of word (string)\n",
    "    :param word_to_vec: a word to vector mapping.\n",
    "    :param seq_len: the fixed length for which the sentence will be mapped to.\n",
    "    :param embedding_dim: the dimension of the w2v embedding\n",
    "    :return: numpy ndarray of shape (seq_len, embedding_dim) with the representation of the sentence\n",
    "    \"\"\"\n",
    "    sentence_embedding = np.zeros((seq_len, embedding_dim))\n",
    "    for i in range(min([len(sent), seq_len])):\n",
    "        word = sent[i]\n",
    "        try:\n",
    "            word_embedding = word_to_vec[word]\n",
    "            sentence_embedding[i] = word_embedding\n",
    "        except:\n",
    "            pass\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936935b1fa05c640",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Loader (Data Manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "866012a66f5f1ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.687062Z",
     "start_time": "2024-05-21T21:21:12.789244Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from processSST import SentimentTreeBank\n",
    "from dataManager import DataManager, TRAIN, VAL, TEST\n",
    "# load the dataset\n",
    "dataset = SentimentTreeBank()\n",
    "# the function that will map a sentence to vector is get_w2v_average\n",
    "sent_func = sentence_to_embedding\n",
    "# The param it takes other than the Sentence object: word2Vec_dic, W2V_EMBEDDING_DIM\n",
    "# initialize the dictionary that map a word to Word2Vec vectors\n",
    "words_list = list(dataset.get_word_counts().keys())\n",
    "word2Vec_dic = create_or_load_slim_w2v(words_list)\n",
    "# We just know that the embedding size of word2Vec is 300\n",
    "sent_func_kwargs = {\"word_to_vec\": word2Vec_dic, \"embedding_dim\": W2V_EMBEDDING_DIM, \"seq_len\": SEQ_LEN}\n",
    "# pass it to the dataManager: batch_size 50\n",
    "data_manager = DataManager(use_sub_phrases=False, \n",
    "                           sentiment_dataset=dataset, \n",
    "                           sent_func=sent_func, sent_func_kwargs=sent_func_kwargs, \n",
    "                           batch_size=50)\n",
    "save_pickle(data_manager, \"TempFiles/SST_DataManager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec74cf7b82a6f1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Training\n",
    "We will use the bidirectional LSTM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2dfca12691da8d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define the model  \n",
    "Regarding LSTM: \n",
    "If we passed in a sentence as list of words, each word as representation, then it will run recurrently word by word for each layer, and the h_n and c_n will be the hidden_layer after the final word. Better shown in a graph. I always get confused by the \"layer of LSTM\" to \"LSTM cell for each word\". each layer will handle an entire sentence, and out put just one h_n for each sentence. For each sentence there are many words so it might be passed to the same layer of LSTM recurrently, each time a word passed to the LSTM cell it will create a h_n, but this h_n will be passed again into the same layer in the next word's step. \n",
    "Also, bi-directional is not two layer of LSTM stacking up. (It might be a little confusing from the graph), it is the same layer, just first time we pass the sentence (list of word) in the front order and second time the reverse order. So each layer will create two h_n and c_n pair. \n",
    "On the other hand, if we pass a sentence in as on single vector (average_word2vec), then it will pass through the LSTM  once (because it's basically just one word) (but sentence of one word is a list of one vector, here we don't even have a list, just one vector, so we basically didn't use the \"recurrent\" attribute at all). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b279b458148977ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.693142Z",
     "start_time": "2024-05-21T21:21:16.690511Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM for sentiment analysis with architecture as described in the exercise description.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            dtype=torch.float64\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=hidden_dim * 2, out_features=1,dtype=torch.float64)\n",
    "        return\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "\n",
    "        :param text: tensor of (batch_size, representation_dim), with avg_word2vec it's probably (batch_size, 300).\n",
    "        but for the real embedding is probably (batch_size, 52, 300), (52, 300) is 52 words, each with 300 dim\n",
    "        embedding.\n",
    "        :return:\n",
    "\n",
    "        Sam's note:\n",
    "        Regarding output of LSTM:\n",
    "        c_n and h_n: cell state and hidden state: both of size (num_layers * num_directions=2, batch_size, hidden_size)\n",
    "            num_layers: how many LSTM cells are stacked together, then each layer have an h_n/c_n to the next layer\n",
    "            num_directions: 2, because it's bi-directional LSTM, for each direction there will be a \"h_n/c_n\", here they just stack output\n",
    "            of both direction on top of each other. \n",
    "        somehow the batch size is not first......\n",
    "        \n",
    "        output_of_lstm: shape: (batch, \"hidden_size_with_shape_of_representation\")\n",
    "            Batch: The entire output for the batch (if there are 50 samples in a batch, there are 50 output), \n",
    "            each output is shape of a sample. \n",
    "            \"hidden_size_with_shape_of_representation\": In our case the shape of a hidden_size_with_shape_of_representation is \n",
    "            (seq_len, hidden_size * num_directions), because representation of a sentence is (seq_len, input_len)\n",
    "                seq_len: num of words regulated to 52 words\n",
    "                input_len: each word is mapped to a vector of shape (300,)\n",
    "                hidden_size: the size of \"h_n\", we set it to be 100\n",
    "                num_directions: 2, because it's bi-directional LSTM, for each direction there will be a \"h_n\", output concatenate them. \n",
    "        Notice that output of lstm can be in a different shape: if the batch is just one vector: If we represent a sentence as one \n",
    "        single vector, then the hidden_size_with_shape_of_representation will be just (hidden_size * num_directions), because\n",
    "        the \"seq_len\" is basically 1. \n",
    "        Why is output this shape?\n",
    "        because as explained above, in one layer of LSTM, it will pass words to itself 52 times recursively. \n",
    "        and pytorch will record all of the \"intermediate\" output that is feed to next iteration-- 52 of them. \n",
    "        Somehow the h_n will record the hidden layer output of each layer (if there is many), but it will not record all 52 of \n",
    "        them. So it is probably the last time. So techniquely, we can either use output_of_lstm[:, -1,:], or simply concatnate\n",
    "        the last layer of h_n's both direction: concat(h_n[-1, :, :], h_n[-2, :, :]). \n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        output_of_lstm, (h_n, c_n) = self.LSTM(text)\n",
    "        last_output = output_of_lstm[:, -1,:]\n",
    "        return self.linear(last_output)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Sam's Note: just use self(text) will return the prediction of the model. We are just adding another layer of sigmoid function here at prediction time.\n",
    "        :param text: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        prediction_before_sigmoid = self(text)\n",
    "        return nn.Sigmoid()(prediction_before_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf1a6a74be575c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "And the function for training a batch, an epoch, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23743c62c707011e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.697666Z",
     "start_time": "2024-05-21T21:21:16.695775Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    This method returns tha accuracy of the predictions, relative to the labels.\n",
    "    You can choose whether to use numpy arrays or tensors here.\n",
    "    I use Tensor here\n",
    "    :param preds: a vector of predictions\n",
    "    :param y: a vector of true labels\n",
    "    :return: scalar value - (<number of accurate predictions> / <number of examples>)\n",
    "    \"\"\"\n",
    "    number_of_accurate_predictions = (torch.round(preds) == y).sum()\n",
    "    number_of_examples = y.shape[0]\n",
    "    return (number_of_accurate_predictions / number_of_examples).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60e38f559252c5fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.703682Z",
     "start_time": "2024-05-21T21:21:16.701706Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(model, optimizer, criterion, batch, device):\n",
    "    \"\"\"\n",
    "    Sam's note: \n",
    "    All the parameters we want to update is automatically set requires_grad=True. So backward() will upgrade the gradients. Because we are just using simple LSTM and Linear from pytorch.nn, so we don't need to worry about this. \n",
    "    Maybe later if we want to parameterize something, we would need to set that newly added tensor's requires_grad=True. Just not something we need to worry about right now. Here are the code to check that\n",
    "    \n",
    "    # Check if the model parameters have requires_grad=True\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'{name}: requires_grad={param.requires_grad}')\n",
    "        \n",
    "    :param model:\n",
    "    :param optimizer:\n",
    "    :param criterion:\n",
    "    :param batch: a list of two tensor: [X, y], shape of X is (batch_size, representation_of_sentence), shape of y is (batch_size, representation_of_target(usually just a number)) -> probably (batch_size,)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # reset the gradient after every backward pass instead of accumulate for the entire epoch\n",
    "    optimizer.zero_grad()\n",
    "    # assign tensor to device, and to the correct type\n",
    "    X = batch[0].to(device).to(torch.float64)\n",
    "    y = batch[1].to(device).to(torch.float64)\n",
    "    # Forward pass the X: also automatically \"use the model to predict y based on X\" (This will be LSTM-Linear)\n",
    "    y_pred = model(X)\n",
    "    # prediction is in (batch_size, 1) shape, but original y is in (batch_size,) shape, so we need to add another dim\n",
    "    y = torch.reshape(y, y_pred.shape)\n",
    "    # get the loss, so that we can preform backpropagation\n",
    "    loss = criterion(input=y_pred, target=y)\n",
    "    # now back propagate the loss to update the parameters of the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # computes loss and accuracy: \n",
    "    # Notice that loss and accuracy and error are different notions, but it's not that complex. just ask GPT\n",
    "    accuracy_value = binary_accuracy(preds=y_pred, y=y)\n",
    "    loss_value = loss.item()\n",
    "    batch_size = batch[0].shape[0]\n",
    "    # Why multiply by batch size? see explanation in train epoch. \n",
    "    return loss_value * batch_size, accuracy_value * batch_size\n",
    "\n",
    "def train_epoch(model, data_iterator, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    This method operates one epoch (pass over the whole train set) of training of the given model,\n",
    "    and returns the accuracy and loss for this epoch\n",
    "    Assume model has method predict.\n",
    "    :param model: the model we're currently training\n",
    "    :param data_iterator: an iterator, iterating over the training data for the model.\n",
    "    :param optimizer: the optimizer object for the training process.\n",
    "    :param criterion: the criterion object for the training process.\n",
    "    \"\"\"\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_sample_size = 0\n",
    "    for batch in data_iterator:\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_sample_size += batch_size\n",
    "        loss, accuracy = train_batch(model, optimizer, criterion, batch, device)\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "    # we divide by total sample size, because the last batch might not be the nnormal batch size. \n",
    "    # so in train batch we multiply loss,accuracy by batch size, and here we devided it by total size. \n",
    "    return total_loss / total_sample_size, total_accuracy / total_sample_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd8ba6-1731-4a4b-bd72-c0c8d19fe8e7",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27788781-b780-44d2-9e5e-a82cc70100b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_batch(model, criterion, batch, device):\n",
    "    \"\"\"\n",
    "    Sam's note:\n",
    "    Setting torch.no_grad(), so pytorch will not track all the tensors with requires_grad=True. (Ususally the parameters tensor\n",
    "    of the model). \n",
    "    There is no need for loss.backward() either. Because loss is in type Tensor. When we do loss.backward() (or even any \n",
    "    tensor.backward()), torch will \"globally\" look at all the tensors with requires_grad=True, and store the amount of grad \n",
    "    needed to be updated when we pass optimizer.step(). (More precisely, it will track it during the forward passing phase, so \n",
    "    that's some computation. \n",
    "    That's why we need torch.no_grad() here. \n",
    "    The only difference between eval batch and train batch is we don't update gradient. We could have put them into the same \n",
    "    function with a flag in function arguments to turn on/off. (But we are using no_grad() to save computation. \n",
    "    :param model: \n",
    "    :param criterion: \n",
    "    :param batch: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        X = batch[0].to(device).to(torch.float64)\n",
    "        y = batch[1].to(device).to(torch.float64)\n",
    "        # Here we remain consistency with train model: We have two options: model(X) or model.predict(X). In our LSTM model, \n",
    "        # model.predict will add a sigmoid layer to the output of the model. \n",
    "        y_pred = model(X)\n",
    "        y = torch.reshape(y, y_pred.shape)\n",
    "        loss = criterion(input=y_pred, target=y)\n",
    "        # compute loss and accuracy\n",
    "        accuracy_value = binary_accuracy(preds=y_pred, y=y)\n",
    "        loss_value = loss.item()\n",
    "        batch_size = batch[0].shape[0]\n",
    "        return loss_value * batch_size, accuracy_value * batch_size\n",
    "\n",
    "def evaluate(model, data_iterator, criterion, device):\n",
    "    \"\"\"\n",
    "    evaluate the model performance on the given data\n",
    "    exactly the same as training, just delete all the line of optimizer\n",
    "    :param model: one of our models..\n",
    "    :param data_iterator: torch data iterator for the relevant subset\n",
    "    :param criterion: the loss criterion used for evaluation\n",
    "    :return: tuple of (average loss over all examples, average accuracy over all examples)\n",
    "    Sam's note: \n",
    "    The only difference between evaluate and train epoch is \n",
    "    \"\"\"\n",
    "    # do we need the with torch.no_grad() here? I don't think so. \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_sample_size = 0\n",
    "    for batch in data_iterator:\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_sample_size += batch_size\n",
    "        loss, accuracy = eval_batch(model, criterion, batch, device)\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "    return total_loss / total_sample_size, total_accuracy / total_sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c69824-178e-4593-886e-96d0bb49797e",
   "metadata": {},
   "source": [
    "## Put Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92328195f33bc07b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:58.428701Z",
     "start_time": "2024-05-21T21:21:16.715772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.6930870433242281, accuracy: 0.48433640868490546\n",
      "epoch 1, loss: 0.6926459958143306, accuracy: 0.4843364073445176\n",
      "epoch 2, loss: 0.6928475360618139, accuracy: 0.4843364084834599\n",
      "Evaluate: loss: 0.6928757190722977, accuracy: 0.4875259885792921\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LSTM(embedding_dim=W2V_EMBEDDING_DIM, hidden_dim=100, n_layers=2, dropout=0.5)\n",
    "# 5 things could be on GPU: model, data, loss_func, optimizer, otehr tensor. model, data must be on same device. The rest is \n",
    "# optional. \n",
    "model.to(device)\n",
    "# Train: data_manager already initialized from above， train_data_iterator is pytorch DataLoader\n",
    "train_data_iterator = data_manager.get_torch_iterator(data_subset=TRAIN)\n",
    "# Define hyper parameters\n",
    "n_epochs, lr, weight_decay = 3, 0.001, 0.0001\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "'''train and evaluate data'''\n",
    "for i in range(n_epochs):\n",
    "    loss, accuracy = train_epoch(model, train_data_iterator, optimizer, criterion, device)\n",
    "    print(f\"epoch {i}, loss: {loss}, accuracy: {accuracy}\")\n",
    "# Evaluate\n",
    "evaluate_data_iterator = data_manager.get_torch_iterator(data_subset=VAL)\n",
    "loss, accuracy = evaluate(model, evaluate_data_iterator, criterion, device)\n",
    "print(f\"Evaluate: loss: {loss}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9094c1d0-7d78-4bd3-ac9e-2bc9cddb72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "model_path = 'TempFiles/biLSTM'\n",
    "torch.save({'epoch': n_epochs,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d954a-0696-4ada-adf2-438a40237a67",
   "metadata": {},
   "source": [
    "# GPU Version\n",
    "For convinience of google colab, if you want to run on google colab, just start running from here.  \n",
    "I will not redo the process of creating data manger. If it's the first time you run, start from the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cfa285-2323-4617-864e-8b7eaff39167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e44e01-afa4-4a19-b13e-431b92ee5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 52\n",
    "W2V_EMBEDDING_DIM = 300\n",
    "\n",
    "def sentence_to_embedding(sent, word_to_vec, seq_len=SEQ_LEN, embedding_dim=W2V_EMBEDDING_DIM):\n",
    "    \"\"\"\n",
    "    this method gets a sentence and a word to vector mapping, and returns a list containing the\n",
    "    words embeddings of the tokens in the sentence.\n",
    "    :param sent: a list of word (string)\n",
    "    :param word_to_vec: a word to vector mapping.\n",
    "    :param seq_len: the fixed length for which the sentence will be mapped to.\n",
    "    :param embedding_dim: the dimension of the w2v embedding\n",
    "    :return: numpy ndarray of shape (seq_len, embedding_dim) with the representation of the sentence\n",
    "    \"\"\"\n",
    "    sentence_embedding = np.zeros((seq_len, embedding_dim))\n",
    "    for i in range(min([len(sent), seq_len])):\n",
    "        word = sent[i]\n",
    "        try:\n",
    "            word_embedding = word_to_vec[word]\n",
    "            sentence_embedding[i] = word_embedding\n",
    "        except:\n",
    "            pass\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e317e1-e229-4b86-9d6c-0bf060a4147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM for sentiment analysis with architecture as described in the exercise description.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            dtype=torch.float64\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=hidden_dim * 2, out_features=1,dtype=torch.float64)\n",
    "        return\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "\n",
    "        :param text: tensor of (batch_size, representation_dim), with avg_word2vec it's probably (batch_size, 300).\n",
    "        but for the real embedding is probably (batch_size, 52, 300), (52, 300) is 52 words, each with 300 dim\n",
    "        embedding.\n",
    "        :return:\n",
    "\n",
    "        Sam's note:\n",
    "        Regarding output of LSTM:\n",
    "        c_n and h_n: cell state and hidden state: both of size (num_layers * num_directions=2, batch_size, hidden_size)\n",
    "            num_layers: how many LSTM cells are stacked together, then each layer have an h_n/c_n to the next layer\n",
    "            num_directions: 2, because it's bi-directional LSTM, for each direction there will be a \"h_n/c_n\", here they just stack output\n",
    "            of both direction on top of each other. \n",
    "        somehow the batch size is not first......\n",
    "        \n",
    "        output_of_lstm: shape: (batch, \"hidden_size_with_shape_of_representation\")\n",
    "            Batch: The entire output for the batch (if there are 50 samples in a batch, there are 50 output), \n",
    "            each output is shape of a sample. \n",
    "            \"hidden_size_with_shape_of_representation\": In our case the shape of a hidden_size_with_shape_of_representation is \n",
    "            (seq_len, hidden_size * num_directions), because representation of a sentence is (seq_len, input_len)\n",
    "                seq_len: num of words regulated to 52 words\n",
    "                input_len: each word is mapped to a vector of shape (300,)\n",
    "                hidden_size: the size of \"h_n\", we set it to be 100\n",
    "                num_directions: 2, because it's bi-directional LSTM, for each direction there will be a \"h_n\", output concatenate them. \n",
    "        Notice that output of lstm can be in a different shape: if the batch is just one vector: If we represent a sentence as one \n",
    "        single vector, then the hidden_size_with_shape_of_representation will be just (hidden_size * num_directions), because\n",
    "        the \"seq_len\" is basically 1. \n",
    "        Why is output this shape?\n",
    "        because as explained above, in one layer of LSTM, it will pass words to itself 52 times recursively. \n",
    "        and pytorch will record all of the \"intermediate\" output that is feed to next iteration-- 52 of them. \n",
    "        Somehow the h_n will record the hidden layer output of each layer (if there is many), but it will not record all 52 of \n",
    "        them. So it is probably the last time. So techniquely, we can either use output_of_lstm[:, -1,:], or simply concatnate\n",
    "        the last layer of h_n's both direction: concat(h_n[-1, :, :], h_n[-2, :, :]). \n",
    "        \n",
    "    \n",
    "        \"\"\"\n",
    "        output_of_lstm, (h_n, c_n) = self.LSTM(text)\n",
    "        last_output = output_of_lstm[:, -1,:]\n",
    "        return self.linear(last_output)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Sam's Note: just use self(text) will return the prediction of the model. We are just adding another layer of sigmoid function here at prediction time.\n",
    "        :param text: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        prediction_before_sigmoid = self(text)\n",
    "        return nn.Sigmoid()(prediction_before_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd6ec1c-d7c4-4348-a8d3-582e3b71b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    This method returns tha accuracy of the predictions, relative to the labels.\n",
    "    You can choose whether to use numpy arrays or tensors here.\n",
    "    I use Tensor here\n",
    "    :param preds: a vector of predictions\n",
    "    :param y: a vector of true labels\n",
    "    :return: scalar value - (<number of accurate predictions> / <number of examples>)\n",
    "    \"\"\"\n",
    "    number_of_accurate_predictions = (torch.round(preds) == y).sum()\n",
    "    number_of_examples = y.shape[0]\n",
    "    return (number_of_accurate_predictions / number_of_examples).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4a6ebc-70e9-4e80-b872-770c24abfe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, optimizer, criterion, batch, device):\n",
    "    \"\"\"\n",
    "    Sam's note: \n",
    "    All the parameters we want to update is automatically set requires_grad=True. So backward() will upgrade the gradients. Because we are just using simple LSTM and Linear from pytorch.nn, so we don't need to worry about this. \n",
    "    Maybe later if we want to parameterize something, we would need to set that newly added tensor's requires_grad=True. Just not something we need to worry about right now. Here are the code to check that\n",
    "    \n",
    "    # Check if the model parameters have requires_grad=True\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'{name}: requires_grad={param.requires_grad}')\n",
    "        \n",
    "    :param model:\n",
    "    :param optimizer:\n",
    "    :param criterion:\n",
    "    :param batch: a list of two tensor: [X, y], shape of X is (batch_size, representation_of_sentence), shape of y is (batch_size, representation_of_target(usually just a number)) -> probably (batch_size,)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # reset the gradient after every backward pass instead of accumulate for the entire epoch\n",
    "    optimizer.zero_grad()\n",
    "    # assign tensor to device, and to the correct type\n",
    "    X = batch[0].to(device).to(torch.float64)\n",
    "    y = batch[1].to(device).to(torch.float64)\n",
    "    # Forward pass the X: also automatically \"use the model to predict y based on X\" (This will be LSTM-Linear)\n",
    "    y_pred = model(X)\n",
    "    # prediction is in (batch_size, 1) shape, but original y is in (batch_size,) shape, so we need to add another dim\n",
    "    y = torch.reshape(y, y_pred.shape)\n",
    "    # get the loss, so that we can preform backpropagation\n",
    "    loss = criterion(input=y_pred, target=y)\n",
    "    # now back propagate the loss to update the parameters of the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # computes loss and accuracy: \n",
    "    # Notice that loss and accuracy and error are different notions, but it's not that complex. just ask GPT\n",
    "    accuracy_value = binary_accuracy(preds=y_pred, y=y)\n",
    "    loss_value = loss.item()\n",
    "    batch_size = batch[0].shape[0]\n",
    "    # Why multiply by batch size? see explanation in train epoch. \n",
    "    return loss_value * batch_size, accuracy_value * batch_size\n",
    "\n",
    "def train_epoch(model, data_iterator, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    This method operates one epoch (pass over the whole train set) of training of the given model,\n",
    "    and returns the accuracy and loss for this epoch\n",
    "    Assume model has method predict.\n",
    "    :param model: the model we're currently training\n",
    "    :param data_iterator: an iterator, iterating over the training data for the model.\n",
    "    :param optimizer: the optimizer object for the training process.\n",
    "    :param criterion: the criterion object for the training process.\n",
    "    \"\"\"\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_sample_size = 0\n",
    "    for batch in data_iterator:\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_sample_size += batch_size\n",
    "        loss, accuracy = train_batch(model, optimizer, criterion, batch, device)\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "    # we divide by total sample size, because the last batch might not be the nnormal batch size. \n",
    "    # so in train batch we multiply loss,accuracy by batch size, and here we devided it by total size. \n",
    "    return total_loss / total_sample_size, total_accuracy / total_sample_size\n",
    "\n",
    "def eval_batch(model, criterion, batch, device):\n",
    "    \"\"\"\n",
    "    Sam's note:\n",
    "    Setting torch.no_grad(), so pytorch will not track all the tensors with requires_grad=True. (Ususally the parameters tensor\n",
    "    of the model). \n",
    "    There is no need for loss.backward() either. Because loss is in type Tensor. When we do loss.backward() (or even any \n",
    "    tensor.backward()), torch will \"globally\" look at all the tensors with requires_grad=True, and store the amount of grad \n",
    "    needed to be updated when we pass optimizer.step(). (More precisely, it will track it during the forward passing phase, so \n",
    "    that's some computation. \n",
    "    That's why we need torch.no_grad() here. \n",
    "    The only difference between eval batch and train batch is we don't update gradient. We could have put them into the same \n",
    "    function with a flag in function arguments to turn on/off. (But we are using no_grad() to save computation. \n",
    "    :param model: \n",
    "    :param criterion: \n",
    "    :param batch: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        X = batch[0].to(device).to(torch.float64)\n",
    "        y = batch[1].to(device).to(torch.float64)\n",
    "        # Here we remain consistency with train model: We have two options: model(X) or model.predict(X). In our LSTM model, \n",
    "        # model.predict will add a sigmoid layer to the output of the model. \n",
    "        y_pred = model(X)\n",
    "        y = torch.reshape(y, y_pred.shape)\n",
    "        loss = criterion(input=y_pred, target=y)\n",
    "        # compute loss and accuracy\n",
    "        accuracy_value = binary_accuracy(preds=y_pred, y=y)\n",
    "        loss_value = loss.item()\n",
    "        batch_size = batch[0].shape[0]\n",
    "        return loss_value * batch_size, accuracy_value * batch_size\n",
    "\n",
    "def evaluate(model, data_iterator, criterion, device):\n",
    "    \"\"\"\n",
    "    evaluate the model performance on the given data\n",
    "    exactly the same as training, just delete all the line of optimizer\n",
    "    :param model: one of our models..\n",
    "    :param data_iterator: torch data iterator for the relevant subset\n",
    "    :param criterion: the loss criterion used for evaluation\n",
    "    :return: tuple of (average loss over all examples, average accuracy over all examples)\n",
    "    Sam's note: \n",
    "    The only difference between evaluate and train epoch is \n",
    "    \"\"\"\n",
    "    # do we need the with torch.no_grad() here? I don't think so. \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_sample_size = 0\n",
    "    for batch in data_iterator:\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_sample_size += batch_size\n",
    "        loss, accuracy = eval_batch(model, criterion, batch, device)\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "    return total_loss / total_sample_size, total_accuracy / total_sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32b7fe-6a9a-4519-9b49-716402888441",
   "metadata": {},
   "source": [
    "## Trainning and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19663eed-0f1c-43bc-b10b-27a0e03c1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This varies with google drive.\n",
    "dataManager_fp = \"TempFiles/SST_DataManager\" # dataManager need to be created first\n",
    "save_model_path = 'TempFiles/biLSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ef4d53-5ecf-4a77-bb6a-1e7ce523809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.692942514257914, accuracy: 0.4843364086655357\n",
      "Evaluate: loss: 0.6932729156927618, accuracy: 0.4875259885792921\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM(embedding_dim=W2V_EMBEDDING_DIM, hidden_dim=100, n_layers=2, dropout=0.5)\n",
    "model.to(device)\n",
    "# load data mananger\n",
    "data_manager = load_pickle(dataManager_fp)\n",
    "train_data_iterator = data_manager.get_torch_iterator(data_subset='train')\n",
    "# Define hyper parameters\n",
    "n_epochs, lr, weight_decay = 1, 0.001, 0.0001\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "'''train and evaluate data'''\n",
    "for i in range(n_epochs):\n",
    "    loss, accuracy = train_epoch(model, train_data_iterator, optimizer, criterion, device)\n",
    "    print(f\"epoch {i}, loss: {loss}, accuracy: {accuracy}\")\n",
    "# Evaluate\n",
    "evaluate_data_iterator = data_manager.get_torch_iterator(data_subset='val')\n",
    "loss, accuracy = evaluate(model, evaluate_data_iterator, criterion, device)\n",
    "print(f\"Evaluate: loss: {loss}, accuracy: {accuracy}\")\n",
    "# save trained model\n",
    "torch.save({'epoch': n_epochs,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb9166-d7bd-40cc-afc7-44cf31358fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

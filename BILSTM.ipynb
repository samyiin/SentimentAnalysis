{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bidirectional LSTM with word2vec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae1bd62ea6e7ac25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Representation\n",
    "The input to our model is a sentence string, we will represent a sentence by word2vec vector. The shape is (300,), type is float64. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88b83d92e571fa1f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:12.004732Z",
     "start_time": "2024-05-21T21:21:12.000769Z"
    }
   },
   "id": "159cc923df98a7ca"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:12.786901Z",
     "start_time": "2024-05-21T21:21:12.009439Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "W2V_EMBEDDING_DIM = 300\n",
    "SEQ_LEN = 52\n",
    "\n",
    "def load_word2vec():\n",
    "    # doesn't work\n",
    "    # word2vec_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "    word2vec_file = 'TempFiles/GoogleNews-vectors-negative300.bin'\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\n",
    "    return word2vec_model\n",
    "\n",
    "def create_or_load_slim_w2v(words_list, cache_w2v=True):\n",
    "    \"\"\"\n",
    "    We are trying to get a smaller word2vec dictionary: word2vec dict only for words which appear in the training dataset.\n",
    "    :param words_list: list of words to use for the w2v dict\n",
    "    :param cache_w2v: whether to save locally the small w2v dictionary\n",
    "    :return: dictionary which maps the known words to their vectors\n",
    "    \"\"\"\n",
    "    w2v_path = \"TempFiles/w2v_dict.pkl\"\n",
    "    if not os.path.exists(w2v_path):\n",
    "        full_w2v = load_word2vec()\n",
    "        w2v_emb_dict = {k: full_w2v[k] for k in words_list if k in full_w2v}\n",
    "        if cache_w2v:\n",
    "            save_pickle(w2v_emb_dict, w2v_path)\n",
    "    else:\n",
    "        w2v_emb_dict = load_pickle(w2v_path)\n",
    "    return w2v_emb_dict\n",
    "\n",
    "\n",
    "def sentence_to_embedding(sent, word_to_vec, seq_len=SEQ_LEN, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    this method gets a sentence and a word to vector mapping, and returns a list containing the\n",
    "    words embeddings of the tokens in the sentence.\n",
    "    :param sent: a list of word (string)\n",
    "    :param word_to_vec: a word to vector mapping.\n",
    "    :param seq_len: the fixed length for which the sentence will be mapped to.\n",
    "    :param embedding_dim: the dimension of the w2v embedding\n",
    "    :return: numpy ndarray of shape (seq_len, embedding_dim) with the representation of the sentence\n",
    "    \"\"\"\n",
    "    sentence_embedding = np.zeros((seq_len, embedding_dim))\n",
    "    for i in range(min([len(sent), seq_len])):\n",
    "        word = sent[i]\n",
    "        try:\n",
    "            word_embedding = word_to_vec[word]\n",
    "            sentence_embedding[i] = word_embedding\n",
    "        except:\n",
    "            pass\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "936935b1fa05c640"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from processSST import SentimentTreeBank\n",
    "from pytorchDataloader import DataManager, TRAIN, VAL, TEST\n",
    "# load the dataset\n",
    "dataset = SentimentTreeBank()\n",
    "# the function that will map a sentence to vector is get_w2v_average\n",
    "sent_func = sentence_to_embedding\n",
    "# The param it takes other than the Sentence object: word2Vec_dic, W2V_EMBEDDING_DIM\n",
    "# initialize the dictionary that map a word to Word2Vec vectors\n",
    "words_list = list(dataset.get_word_counts().keys())\n",
    "word2Vec_dic = create_or_load_slim_w2v(words_list)\n",
    "# We just know that the embedding size of word2Vec is 300\n",
    "sent_func_kwargs = {\"word_to_vec\": word2Vec_dic, \"embedding_dim\": W2V_EMBEDDING_DIM, \"seq_len\": SEQ_LEN}\n",
    "# pass it to the dataManager\n",
    "data_manager = DataManager(use_sub_phrases=False, \n",
    "                           sentiment_dataset=dataset, \n",
    "                           sent_func=sent_func, sent_func_kwargs=sent_func_kwargs, \n",
    "                           batch_size=50)\n",
    "train_dataloader = data_manager.get_torch_iterator(data_subset=\"train\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.687062Z",
     "start_time": "2024-05-21T21:21:12.789244Z"
    }
   },
   "id": "866012a66f5f1ac0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "We will use the bidirectional LSTM architecture"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8ec74cf7b82a6f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the model  \n",
    "Regarding LSTM: \n",
    "If we passed in a sentence as list of words, each word as representation, then it will run recurrently word by word for each layer, and the h_n and c_n will be the hidden_layer after the final word. Better shown in a graph. I always get confused by the \"layer of LSTM\" to \"LSTM cell for each word\". each layer will handle an entire sentence, and out put just one h_n for each sentence. For each sentence there are many words so it might be passed to the same layer of LSTM recurrently, each time a word passed to the LSTM cell it will create a h_n, but this h_n will be passed again into the same layer in the next word's step. \n",
    "Also, bi-directional is not two layer of LSTM stacking up. (It might be a little confusing from the graph), it is the same layer, just first time we pass the sentence (list of word) in the front order and second time the reverse order. So each layer will create two h_n and c_n pair. \n",
    "On the other hand, if we pass a sentence in as on single vector (average_word2vec), then it will pass through the LSTM  once (because it's basically just one word) (but sentence of one word is a list of one vector, here we don't even have a list, just one vector, so we basically didn't use the \"recurrent\" attribute at all). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa2dfca12691da8d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM for sentiment analysis with architecture as described in the exercise description.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            dtype=torch.float64\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=hidden_dim * 2, out_features=1,dtype=torch.float64)\n",
    "        return\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "\n",
    "        :param text: tensor of (batch_size, representation_dim), with avg_word2vec it's probably (batch_size, 300).\n",
    "        but for the real embedding is probably (batch_size, 52, 300), (52, 300) is 52 words, each with 300 dim\n",
    "        embedding.\n",
    "        :return:\n",
    "\n",
    "        Sam's note:\n",
    "        Regarding output of LSTM:\n",
    "        c_n and h_n: cell state and hidden state: both of size (num_layers * num_directions=2, batch_size, hidden_size)\n",
    "        somehow the batch size is not first......\n",
    "        output_of_lstm: the entire output for the batch (if there are 50 samples in a batch, there are 50 output), each\n",
    "        output is size of (seq_len(num of words, regulated to 52 words),hidden_size * num_directions=2) \n",
    "        \n",
    "        because as explained above, in one layer of LSTM, it will pass words to itself 52 times recursively. \n",
    "        and pytorch will record all of the \"intermediate\" output that is feed to next iteration-- 52 of them\n",
    "        \n",
    "        if it's bi directional, it will concatenate the resulting hidden state of both direction, for each sample. \n",
    "\n",
    "        \"\"\"\n",
    "        output_of_lstm, (h_n, c_n) = self.LSTM(text)\n",
    "        last_output = output_of_lstm[:, -1,:]\n",
    "        return self.linear(last_output)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        Sam's Note: just use self(text) will return the prediction of the model. We are just adding another layer of sigmoid function here at prediction time.\n",
    "        :param text: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        prediction_before_sigmoid = self(text)\n",
    "        return nn.Sigmoid()(prediction_before_sigmoid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.693142Z",
     "start_time": "2024-05-21T21:21:16.690511Z"
    }
   },
   "id": "b279b458148977ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the function for training a batch, an epoch, etc. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5edf1a6a74be575c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    This method returns tha accuracy of the predictions, relative to the labels.\n",
    "    You can choose whether to use numpy arrays or tensors here.\n",
    "    I use Tensor here\n",
    "    :param preds: a vector of predictions\n",
    "    :param y: a vector of true labels\n",
    "    :return: scalar value - (<number of accurate predictions> / <number of examples>)\n",
    "    \"\"\"\n",
    "    number_of_accurate_predictions = (torch.round(preds) == y).sum()\n",
    "    number_of_examples = y.shape[0]\n",
    "    return (number_of_accurate_predictions / number_of_examples).item()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.697666Z",
     "start_time": "2024-05-21T21:21:16.695775Z"
    }
   },
   "id": "23743c62c707011e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train_batch(model, optimizer, criterion, batch):\n",
    "    \"\"\"\n",
    "    Sam's note: \n",
    "    All the parameters we want to update is automatically set requires_grad=True. So backward() will upgrade the gradients. Because we are just using simple LSTM and Linear from pytorch.nn, so we don't need to worry about this. \n",
    "    Maybe later if we want to parameterize something, we would need to set that newly added tensor's requires_grad=True. Just not something we need to worry about right now. Here are the code to check that\n",
    "    \n",
    "    # Check if the model parameters have requires_grad=True\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'{name}: requires_grad={param.requires_grad}')\n",
    "        \n",
    "    :param model:\n",
    "    :param optimizer:\n",
    "    :param criterion:\n",
    "    :param batch: a list of two tensor: [X, y], shape of X is (batch_size, representation_of_sentence), shape of y is (batch_size, representation_of_target(usually just a number)) -> probably (batch_size,)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # reset the gradient after every backward pass instead of accumulate for the entire epoch\n",
    "    optimizer.zero_grad()\n",
    "    # assign tensor to device, and to the correct type\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X = batch[0].to(device).to(torch.float64)\n",
    "    y = batch[1].to(device).to(torch.float64)\n",
    "    # Forward pass the X: also automatically \"use the model to predict y based on X\" (This will be LSTM-Linear)\n",
    "    y_pred = model(X)\n",
    "    # prediction is in (batch_size, 1) shape, but original y is in (batch_size,) shape, so we need to add another dim\n",
    "    y = torch.reshape(y, y_pred.shape)\n",
    "    # get the loss, so that we can preform backpropagation\n",
    "    '''here user prediction before sigmoid for criteria, why?'''\n",
    "    loss = criterion(input=y_pred, target=y)\n",
    "    # now back propagate the loss to update the parameters of the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # computes loss and accuracy: why use sigmoid here?\n",
    "    accuracy_value = binary_accuracy(preds=y_pred, y=y)\n",
    "    loss_value = loss.item()\n",
    "    batch_size = batch[0].shape[0]\n",
    "    return loss_value * batch_size, accuracy_value * batch_size\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.703682Z",
     "start_time": "2024-05-21T21:21:16.701706Z"
    }
   },
   "id": "60e38f559252c5fd"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_epoch(model, data_iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    This method operates one epoch (pass over the whole train set) of training of the given model,\n",
    "    and returns the accuracy and loss for this epoch\n",
    "    Assume model has method predict.\n",
    "    :param model: the model we're currently training\n",
    "    :param data_iterator: an iterator, iterating over the training data for the model.\n",
    "    :param optimizer: the optimizer object for the training process.\n",
    "    :param criterion: the criterion object for the training process.\n",
    "    \"\"\"\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_sample_size = 0\n",
    "    for batch in data_iterator:\n",
    "        batch_size = batch[0].shape[0]\n",
    "        total_sample_size += batch_size\n",
    "        loss, accuracy = train_batch(model, optimizer, criterion, batch)\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "    return total_loss / total_sample_size, total_accuracy / total_sample_size\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.708280Z",
     "start_time": "2024-05-21T21:21:16.705994Z"
    }
   },
   "id": "68952e9360e0d69a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_model(model_name, model, data_manager: DataManager, n_epochs, lr, weight_decay=0.):\n",
    "    \"\"\"\n",
    "    Runs the full training procedure for the given model. The optimization should be done using the Adam\n",
    "    optimizer with all parameters but learning rate and weight decay set to default.\n",
    "    :param model_name: name of model\n",
    "    :param model: module of one of the models implemented in the exercise\n",
    "    :param data_manager: the DataManager object\n",
    "    :param n_epochs: number of times to go over the whole training set\n",
    "    :param lr: learning rate to be used for optimization\n",
    "    :param weight_decay: parameter for l2 regularization\n",
    "    \"\"\"\n",
    "    train_data_iterator = data_manager.get_torch_iterator(data_subset=TRAIN)\n",
    "    evaluate_data_iterator = data_manager.get_torch_iterator(data_subset=VAL)\n",
    "    '''lr and weight_decay should set to default? Then what's parameter for?'''\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = F.binary_cross_entropy_with_logits\n",
    "    '''train and evaluate data'''\n",
    "    for i in range(n_epochs):\n",
    "        loss, accuracy = train_epoch(model, train_data_iterator, optimizer, criterion)\n",
    "        # loss, accuracy = evaluate(model, evaluate_data_iterator, criterion)\n",
    "        # evaluate_stats_recorder.update_error_and_accuracy(epoch_number=i, loss=loss,\n",
    "        #                                                   accuracy=accuracy)\n",
    "    # '''save trained model'''\n",
    "    # save_model(model, \"{}\".format(model_name), n_epochs, optimizer)\n",
    "    # save_pickle(data_manager, \"{}_DataManager\".format(model_name))\n",
    "    # save_pickle(training_stats_recorder, \"{}_stats_{}\".format(model_name, TRAIN))\n",
    "    # save_pickle(evaluate_stats_recorder, \"{}_stats_{}\".format(model_name, VAL))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:16.717189Z",
     "start_time": "2024-05-21T21:21:16.711314Z"
    }
   },
   "id": "3bac3497553f1f23"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "model = LSTM(embedding_dim=W2V_EMBEDDING_DIM, hidden_dim=100, n_layers=2, dropout=0.5)\n",
    "# data_manager already initialized from above\n",
    "train_model('model_name', model, data_manager=data_manager, n_epochs=1, lr=0.001, weight_decay=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T21:21:58.428701Z",
     "start_time": "2024-05-21T21:21:16.715772Z"
    }
   },
   "id": "92328195f33bc07b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44543bfaa8c788cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# General preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520a55c090bc9f30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:39.147882Z",
     "start_time": "2024-05-08T17:56:39.127564Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed58fda17a1bc1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# dataLoader\n",
    "We will first present some examples for how to handle classes in the dataLoder file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed188c5455689139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:40.866299Z",
     "start_time": "2024-05-08T17:56:39.138614Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from processSST import Sentence, SentimentTreeBank, SentimentTreeNode\n",
    "from processSST import NEUTRAL_SENTIMENT, POSITIVE_SENTIMENT, NEGATIVE_SENTIMENT\n",
    "import random\n",
    "\n",
    "dataset = SentimentTreeBank()\n",
    "# this will return a list of Sentence object, Sentence object contains a tree of SentimentTreeNode object. \n",
    "sentences = dataset.get_train_set()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c8b95e4b2215f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:40.873309Z",
     "start_time": "2024-05-08T17:56:40.866078Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29167\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0].sentiment_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:40.880932Z",
     "start_time": "2024-05-08T17:56:40.875830Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 14, 54, 109, 113]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_negated_polarity_examples(sentences_list, num_examples=None, choose_random=False):\n",
    "    \"\"\"\n",
    "    Returns the indices of the sentences in sentences_list which have subphrase in the second level with\n",
    "    sentiment polarity different than the whole sentence polarity.\n",
    "    :param sentences_list: list of Sentence objects\n",
    "    :param num_examples: number of examples to return, if None all of them are returned\n",
    "    :param choose_random: relevant only if num_examples is lower than the number of exisitng negated\n",
    "    polarity examples in sentences_list\n",
    "    \"\"\"\n",
    "\n",
    "    if num_examples is None:\n",
    "        num_examples = len(sentences_list)  # take all possible sentences\n",
    "\n",
    "    def is_polarized(sent: Sentence):\n",
    "        if sent.sentiment_class == NEUTRAL_SENTIMENT:\n",
    "            return False\n",
    "        else:\n",
    "            root_polarity = sent.sentiment_class\n",
    "            for child in sent.root.children:\n",
    "                if child.sentiment_class == 1 - root_polarity:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "    indexed_sentences = list(enumerate(sentences_list)) # in format of [(index, sentences), ...]\n",
    "    negated_sentences = list(filter(lambda s: is_polarized(s[1]), indexed_sentences))\n",
    "    negated_sentences_indices = [i for i, s in negated_sentences]\n",
    "    \n",
    "    # select number of samples we want\n",
    "    if len(negated_sentences) <= num_examples:\n",
    "        return negated_sentences_indices\n",
    "    else:\n",
    "        if choose_random:\n",
    "            random.shuffle(negated_sentences_indices)\n",
    "        return negated_sentences_indices[:num_examples]\n",
    "get_negated_polarity_examples(sentences, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e8111b4485fcd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:40.887948Z",
     "start_time": "2024-05-08T17:56:40.881783Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['charms'], ['co'], ['stars']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentiment_words(sent: Sentence):\n",
    "    sent_polarity = sent.sentiment_class\n",
    "    return [node for node in sent.get_leaves() if node.sentiment_class == sent_polarity]\n",
    "sent = sentences[54]\n",
    "nodes = get_sentiment_words(sent)\n",
    "[node.text for node in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0bbc96e5061fdac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:40.990555Z",
     "start_time": "2024-05-08T17:56:40.940977Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 32, 37, 76, 198]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rare_words_examples(sentences_list, dataset: SentimentTreeBank,\n",
    "                            num_sentences=50):\n",
    "    \"\"\"\n",
    "    Computes for each sentence in sentences the maximal train frequency of sentiment word, where sentiment\n",
    "    word is a word which is labeled with either positive or negative sentiment value, and returns the\n",
    "    indices of the <num_sentences> sentences with lowest value.\n",
    "    :param sentences_list: list of Sentence objects\n",
    "    :param dataset: the SentimentTreebank datset object\n",
    "    :param num_sentences: number of sentences to return\n",
    "    :return: list of ints representing the indices of the chosen sentences out of the input sentences_list\n",
    "    \"\"\"\n",
    "    word_counts = dataset.get_train_word_counts()\n",
    "\n",
    "    def get_count(word_node: SentimentTreeNode):\n",
    "        word_text = word_node.text[0]\n",
    "        if word_text in word_counts:\n",
    "            return word_counts[word_text]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    indexed_sentences = list(enumerate(sentences_list))\n",
    "    indexed_sentences = list(filter(lambda s: len(get_sentiment_words(s[1])) > 0, indexed_sentences))\n",
    "    # sort sentence by the highest of the count of each of its words, the smaller one is in the front\n",
    "    indexed_sentences = sorted(indexed_sentences, key=lambda s: max([get_count(node) for node in\n",
    "                                                                     get_sentiment_words(s[1])]))\n",
    "    indices = [i for i, s in indexed_sentences]\n",
    "    return indices[:num_sentences]\n",
    "\n",
    "get_rare_words_examples(sentences, dataset, num_sentences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d3b2ce1d7bce2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# dataManager\n",
    "Let's give some example for how to use the dataManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cf44e46330502",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## encode sentence as one hot\n",
    "The idea is to get a ordered list of all the words, and then simply encode each word as a one hot vector of the size of list of all words.  \n",
    "Then we simply encode the sentence by the average of the one hot vectors of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cd8b46620f24e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:41.165906Z",
     "start_time": "2024-05-08T17:56:40.988927Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataManager import DataManager\n",
    "from processSST import Sentence\n",
    "def get_one_hot(size, ind):\n",
    "    \"\"\"\n",
    "    this method returns a one-hot vector of the given size, where the 1 is placed in the ind entry.\n",
    "    we can safely assume that ind < size\n",
    "    :param size: the size of the vector\n",
    "    :param ind: the entry index to turn to 1\n",
    "    :return: numpy ndarray which represents the one-hot vector\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((size,))\n",
    "    one_hot[ind] = 1\n",
    "    return one_hot\n",
    "\n",
    "def get_word_to_ind(words_list):\n",
    "    \"\"\"\n",
    "    this function gets a list of words, and returns a mapping between\n",
    "    words to their index.\n",
    "    Can word repeat? Assume no? assume yes\n",
    "    :param words_list: a list of words\n",
    "    :return: the dictionary mapping words to the index\n",
    "    \"\"\"\n",
    "    word_to_index = {}\n",
    "    for i in range(len(words_list)):\n",
    "        word = words_list[i]\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = i\n",
    "    return word_to_index\n",
    "\n",
    "def average_one_hots(sent: Sentence, word_to_ind: dict):\n",
    "    \"\"\"\n",
    "    this method gets a sentence, and a mapping between words to indices, and returns the average\n",
    "    one-hot embedding of the tokens in the sentence.\n",
    "    assume all word in sent.text is in word_to_ind.key\n",
    "    :param sent: a sentence object.\n",
    "    :param word_to_ind: a mapping between words to indices\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    text = sent.text\n",
    "    size_of_one_hot = max(word_to_ind.values()) + 1\n",
    "    all_one_hot = np.zeros(size_of_one_hot)\n",
    "    for word in text:\n",
    "        ind_of_word = word_to_ind[word]\n",
    "        all_one_hot += get_one_hot(size_of_one_hot, ind_of_word)\n",
    "    return all_one_hot / len(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc9e2e42493c709",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:44.979591Z",
     "start_time": "2024-05-08T17:56:41.178675Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataManager.DataManager at 0x1179b2ef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataManager import DataManager\n",
    "# load the dataset\n",
    "dataset = SentimentTreeBank()\n",
    "# the embedding function is average one hot\n",
    "sent_func = average_one_hots\n",
    "# get the dictionary that map word to index\n",
    "words_list = list(dataset.get_word_counts().keys())\n",
    "word_to_ind = get_word_to_ind(words_list)\n",
    "# define the parameters for the embedding function\n",
    "sent_func_kwargs = {\"word_to_ind\":word_to_ind }\n",
    "# pass it to the dataManager\n",
    "data_manager = DataManager(use_sub_phrases=False, \n",
    "                                       sentiment_dataset=dataset, \n",
    "                                       sent_func=sent_func, sent_func_kwargs=sent_func_kwargs, \n",
    "                                       batch_size=50)\n",
    "data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3208b64aa0800dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:44.985093Z",
     "start_time": "2024-05-08T17:56:44.980420Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1278a38e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the pyTorch DataLoader\n",
    "train_set_dataloader = data_manager.get_torch_iterator(data_subset=\"train\")\n",
    "train_set_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73de8ee1f90272",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Encode sentence by Word2Vec\n",
    "The idea is similar to above: we first embed a word into a vector, then we will calculate the embedding of the sentence by the average of the embedding of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c082c2c6472659d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First we do some preparation: if Word2Vec already exist, then we will load it, else we will download it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66f0fe862aa4ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Need to use gensim 4.3.2 (current version) will \"from scipy.linalg import get_blas_funcs, triu\", and triu is removed from scipy 1.12. And we can't install scipy 1.11. I tried to download the file and write a load function for it, but there is little information on the internet, everyone is using gensim.  \n",
    "So eventually my solution is to use gensim 4.3.2 and python 3.10. We can install scipy 1.11.0 with python 3.10, and it solves the problem. besides, gensim.downloader.load(\"word2vec-google-news-300\") seems stop working. So we will have to download the file from\n",
    " https://code.google.com/archive/p/word2vec/    \n",
    " (1.5 GB) and unzip it (not sure if it's necessary), and then use from gensim.models import KeyedVectors to solve this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b91872a7609652cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T18:06:15.209670Z",
     "start_time": "2024-05-08T18:06:15.201772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def load_word2vec():\n",
    "    # doesn't work\n",
    "    # word2vec_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "    word2vec_file = 'TempFiles/GoogleNews-vectors-negative300.bin'\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\n",
    "    return word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c1fdbc13d09564d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T18:06:15.955035Z",
     "start_time": "2024-05-08T18:06:15.947308Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def create_or_load_slim_w2v(words_list, cache_w2v=True):\n",
    "    \"\"\"\n",
    "    We are trying to get a smaller word2vec dictionary: word2vec dict only for words which appear in the training dataset.\n",
    "    :param words_list: list of words to use for the w2v dict\n",
    "    :param cache_w2v: whether to save locally the small w2v dictionary\n",
    "    :return: dictionary which maps the known words to their vectors\n",
    "    \"\"\"\n",
    "    w2v_path = \"TempFiles/w2v_dict.pkl\"\n",
    "    if not os.path.exists(w2v_path):\n",
    "        full_w2v = load_word2vec()\n",
    "        w2v_emb_dict = {k: full_w2v[k] for k in words_list if k in full_w2v}\n",
    "        if cache_w2v:\n",
    "            save_pickle(w2v_emb_dict, w2v_path)\n",
    "    else:\n",
    "        w2v_emb_dict = load_pickle(w2v_path)\n",
    "    return w2v_emb_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e868e7a138e91c3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T18:06:18.174664Z",
     "start_time": "2024-05-08T18:06:18.166588Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_w2v_average(sent, word_to_vec, embedding_dim):\n",
    "    \"\"\"\n",
    "    This method gets a sentence and returns the average word embedding of the words consisting\n",
    "    the sentence.\n",
    "    :param sent: the sentence object\n",
    "    :param word_to_vec: a dictionary mapping words to their vector embeddings\n",
    "    :param embedding_dim: the dimension of the word embedding vectors, we need this parameter because we might meet the situation that all the words in the sentence does not have a word2vec embedding, then we will manually pass in the size of the embedding, and set it all to zero\n",
    "    :return The average embedding vector as numpy ndarray.\n",
    "    \"\"\"\n",
    "    word_embeddings = []\n",
    "    for word in sent.text:\n",
    "        try:\n",
    "            word_embedding = word_to_vec[word]\n",
    "            # we assume word_embedding is of dimension embedding_dim\n",
    "            word_embeddings.append(word_embedding)\n",
    "            # average without unknown\n",
    "        except:\n",
    "            pass\n",
    "    if len(word_embeddings) == 0:\n",
    "        default_sentence_embedding = np.zeros(embedding_dim)\n",
    "        return default_sentence_embedding\n",
    "    return np.mean(word_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2db552a6395ee47f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T18:06:34.301702Z",
     "start_time": "2024-05-08T18:06:18.735209Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from dataManager import DataManager\n",
    "# load the dataset\n",
    "dataset = SentimentTreeBank()\n",
    "# the function that will map a sentence to vector is get_w2v_average\n",
    "sent_func = get_w2v_average\n",
    "# The param it takes other than the Sentence object: word2Vec_dic, W2V_EMBEDDING_DIM\n",
    "# initialize the dictionary that map a word to Word2Vec vectors\n",
    "words_list = list(dataset.get_word_counts().keys())\n",
    "word2Vec_dic = create_or_load_slim_w2v(words_list)\n",
    "# We just know that the embedding size of word2Vec is 300\n",
    "W2V_EMBEDDING_DIM = 300\n",
    "sent_func_kwargs = {\"word_to_vec\": word2Vec_dic, \"embedding_dim\": W2V_EMBEDDING_DIM}\n",
    "# pass it to the dataManager\n",
    "data_manager = DataManager(use_sub_phrases=False, \n",
    "                                       sentiment_dataset=dataset, \n",
    "                                       sent_func=sent_func, sent_func_kwargs=sent_func_kwargs, \n",
    "                                       batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99430417eda1899a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T18:06:38.601436Z",
     "start_time": "2024-05-08T18:06:38.584266Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x3a011b790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then as above, let's see the DataLoder for training set\n",
    "train_set_dataloader = data_manager.get_torch_iterator(data_subset=\"train\")\n",
    "train_set_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48ef56401cfc9b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## sequence of embeddings\n",
    "We will try another way to embed a sentence, fix a length SEQ_LEN = 52, we will embed every word in a sentence by word2vec, and then we will embed a sentence by a list of all the word2vec embeddings. And if the sentence is longer than SEQ_LEN, we will crop it, if it is shorter than SEQ_LEN = 52, we will pad the rest with zero word2vec embeddings. So the embedding of a sentence will be (SEQ_LEN = 52, len_word2vec = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56405cedb6ce8c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:56:50.891116Z",
     "start_time": "2024-05-08T17:56:50.882460Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 52\n",
    "def sentence_to_embedding(sent, word_to_vec, seq_len=SEQ_LEN, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    this method gets a sentence and a word to vector mapping, and returns a list containing the\n",
    "    words embeddings of the tokens in the sentence.\n",
    "    :param sent: a sentence object\n",
    "    :param word_to_vec: a word to vector mapping.\n",
    "    :param seq_len: the fixed length for which the sentence will be mapped to.\n",
    "    :param embedding_dim: the dimension of the w2v embedding\n",
    "    :return: numpy ndarray of shape (seq_len, embedding_dim) with the representation of the sentence\n",
    "    \"\"\"\n",
    "    sentence_embedding = np.zeros((seq_len, embedding_dim))\n",
    "    for i in range(min([len(sent.text), seq_len])):\n",
    "        word = sent.text[i]\n",
    "        try:\n",
    "            word_embedding = word_to_vec[word]\n",
    "            sentence_embedding[i] = word_embedding\n",
    "        except:\n",
    "            pass\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "918e7e6a5b40ccd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T18:08:32.546790Z",
     "start_time": "2024-05-08T18:08:26.056629Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2a3e122f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataManager import DataManager\n",
    "# load the dataset\n",
    "dataset = SentimentTreeBank()\n",
    "# the function that will map a sentence to vector is get_w2v_average\n",
    "sent_func = sentence_to_embedding\n",
    "# The param it takes other than the Sentence object: word2Vec_dic, W2V_EMBEDDING_DIM\n",
    "# initialize the dictionary that map a word to Word2Vec vectors\n",
    "words_list = list(dataset.get_word_counts().keys())\n",
    "word2Vec_dic = create_or_load_slim_w2v(words_list)\n",
    "# We just know that the embedding size of word2Vec is 300\n",
    "W2V_EMBEDDING_DIM = 300\n",
    "SEQ_LEN = 52\n",
    "sent_func_kwargs = {\"seq_len\": SEQ_LEN, \"word_to_vec\": word2Vec_dic, \"embedding_dim\": W2V_EMBEDDING_DIM}\n",
    "# pass it to the dataManager\n",
    "data_manager = DataManager(use_sub_phrases=False, \n",
    "                                       sentiment_dataset=dataset, \n",
    "                                       sent_func=sent_func, sent_func_kwargs=sent_func_kwargs, \n",
    "                                       batch_size=50)\n",
    "# then as above, let's see the DataLoder for training set\n",
    "train_set_dataloader = data_manager.get_torch_iterator(data_subset=\"train\")\n",
    "train_set_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2910ad2a4712d508",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
